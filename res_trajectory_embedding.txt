worker-minor-4
/usr/bin/python3
Wed May  7 17:15:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:02:00.0  On |                  N/A |
|  0%   35C    P8              15W / 300W |      0MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
SHELL=/bin/bash
SLURM_JOB_USER=xian
SLURM_TASKS_PER_NODE=1
SLURM_JOB_UID=33124
TERM_PROGRAM_VERSION=3.4
SLURM_TASK_PID=4902
TMUX=/tmp/tmux-33124/default,135456,4
SLURM_JOB_GPUS=0
SLURM_LOCALID=0
SLURM_SUBMIT_DIR=/home/wiss/xian/Python_code/oleg/trajectory_embedding_learning/trajectory-subspace-clustering
HOSTNAME=worker-minor-4
SLURMD_NODENAME=worker-minor-4
SLURM_JOB_START_TIME=1746630954
HYDRA_LAUNCHER_EXTRA_ARGS=--external-launcher
SLURM_CLUSTER_NAME=cluster
SLURM_JOB_END_TIME=1747145754
SLURM_CPUS_ON_NODE=2
SLURM_JOB_CPUS_PER_NODE=2
SLURM_GPUS_ON_NODE=1
PRTE_MCA_plm_slurm_args=--external-launcher
PWD=/home/wiss/xian/Python_code/oleg/trajectory_embedding_learning/trajectory-subspace-clustering
SLURM_GTIDS=0
LOGNAME=xian
XDG_SESSION_TYPE=tty
SLURM_JOB_PARTITION=all
SLURM_JOB_NUM_NODES=1
SLURM_JOBID=60350
SLURM_JOB_QOS=normal
I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS=--external-launcher
HOME=/home/wiss/xian
LANG=en_US.UTF-8
SLURM_PROCID=0
TMPDIR=/tmp
SLURM_NTASKS=1
SLURM_TOPOLOGY_ADDR=worker-minor-4
SSH_CONNECTION=10.153.7.234 58816 10.153.51.197 22
HYDRA_BOOTSTRAP=slurm
SLURM_TOPOLOGY_ADDR_PATTERN=node
CUDA_VISIBLE_DEVICES=0
XDG_SESSION_CLASS=user
TERM=tmux-256color
USER=xian
TMUX_PANE=%5
SLURM_NODELIST=worker-minor-4
ENVIRONMENT=BATCH
SLURM_JOB_ACCOUNT=seidl
SLURM_PRIO_PROCESS=0
SLURM_NPROCS=1
SHLVL=3
SLURM_NNODES=1
XDG_SESSION_ID=174
SLURM_SUBMIT_HOST=madeira.dbs.ifi.lmu.de
XDG_RUNTIME_DIR=/run/user/33124
SLURM_JOB_ID=60350
SLURM_NODEID=0
SSH_CLIENT=141.84.18.4 56250 22
XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop
SLURM_CONF=/etc/slurm/slurm.conf
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
SLURM_JOB_NAME=trajectory_embedding
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/33124/bus
SSH_TTY=/dev/pts/0
OMPI_MCA_plm_slurm_args=--external-launcher
SLURM_JOB_GID=12000
OLDPWD=/home/wiss/xian/Python_code/oleg/trajectory_embedding_learning/trajectory-subspace-clustering/models
SLURM_JOB_NODELIST=worker-minor-4
TERM_PROGRAM=tmux
I_MPI_HYDRA_BOOTSTRAP=slurm
_=/usr/bin/env
/home/wiss/xian/Python_code/oleg/trajectory_embedding_learning/trajectory-subspace-clustering/./playground/inference_save_feature_embeddings.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(load_path, map_location=target_device)
/home/wiss/xian/venvs/subspace_clustering_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/wiss/xian/venvs/subspace_clustering_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/wiss/xian/venvs/subspace_clustering_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
